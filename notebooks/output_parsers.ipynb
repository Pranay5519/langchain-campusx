{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dc3147",
   "metadata": {},
   "source": [
    "# Str Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69cef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 5-line summary of the text:\n",
      "\n",
      "A black hole is a region in space where gravity is so strong that nothing, including light, can escape. It is formed when a massive star collapses and its gravity warps spacetime around it, creating an event horizon that marks the point of no return. Black holes have several characteristics, including a singularity at their center, incredibly strong gravitational fields, and no emission of radiation or light. There are four types of black holes, ranging from stellar to supermassive, each with unique properties and effects on spacetime. Continued research and observation of black holes will help us better understand these enigmatic objects and their role in the universe.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = template1 | model | parser | template2 | model | parser\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279797b",
   "metadata": {},
   "source": [
    "# JSON output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0adfce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser ,JsonOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "model = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.7\n",
    ")\n",
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd73f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me 5 facts about black hole \\n Return a JSON object.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.format(topic='black hole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af011aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed55a90",
   "metadata": {},
   "source": [
    "# Structured Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e304f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.output_parsers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructuredOutputParser, ResponseSchema\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[32m      6\u001b[39m load_dotenv()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.output_parsers'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "\n",
    "# Define the model\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f772cdc",
   "metadata": {},
   "source": [
    "# Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84d81628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Emily Carter' age=25 city='Vancouver'\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "\n",
    "# Define the model\n",
    "model = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.7\n",
    ")\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city the person belongs to')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "final_result = chain.invoke({'place':'canada'})\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "420f5ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Anura Wijeyasinghe', 25, 'Kandy')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.name, final_result.age, final_result.city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fa3a7",
   "metadata": {},
   "source": [
    "# Nested pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fccabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\Desktop\\VS_CODE\\LangChainModels\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "\n",
    "# Define the model\n",
    "model = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63a9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtopic(BaseModel):\n",
    "    name: str = Field(description=\"Name of the model type (e.g., Vision Models, Text Models)\") \n",
    "    advantages: List[str] = Field(description=\"List of advantages of this model type\") \n",
    "    disadvantages: List[str] = Field(description=\"List of disadvantages of this model type\") \n",
    "    conclusion: str = Field(description=\"Short conclusion about this model type\") \n",
    "    \n",
    "class Topic(BaseModel):\n",
    "    title: str = Field(description=\"Main topic name\") \n",
    "    intro: str = Field(description=\"Definition or introduction of the topic\")\n",
    "    categories: List[Subtopic] = Field(description=\"Different model types under this topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "647a9e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Large Language Models (LLMs)' intro='Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to perform a wide range of tasks, including text generation, translation, summarization, and question-answering. They leverage deep learning techniques to understand and generate human-like text.' categories=[Subtopic(name='Text Models', advantages=['Natural language understanding and generation', 'Versatile for various text-based tasks', 'High accuracy in language-related tasks'], disadvantages=['Limited to text processing, lacking visual/audio capabilities', 'Potential for generating biased or inaccurate content', 'Requires extensive training data for optimal performance'], conclusion='Text models are essential for language-based applications but have limitations in handling non-text data.'), Subtopic(name='Vision Models', advantages=['Capable of processing and understanding visual data', 'Effective in image recognition and description tasks', 'Can integrate with text models for multimodal applications'], disadvantages=['Require large annotated image datasets for training', 'Higher computational resources compared to text models', 'Potential for biased or inaccurate visual interpretations'], conclusion='Vision models expand the capabilities of LLMs in multimedia tasks but demand significant resources.'), Subtopic(name='Code Models', advantages=['Specialized in understanding and generating programming code', 'Assist in debugging, code completion, and documentation', 'Support multiple programming languages'], disadvantages=['May produce syntactically correct but semantically incorrect code', 'Limited to code-related tasks, not general-purpose', 'Requires domain-specific training data'], conclusion='Code models are invaluable for developers but have constraints in broader applications.'), Subtopic(name='Multimodal Models', advantages=['Integrate text, images, audio, and video processing', 'Enable complex applications like virtual assistants and content creation', 'Provide a more holistic understanding of data'], disadvantages=['High computational complexity and resource demands', 'Challenging to train and fine-tune effectively', 'Potential for errors in cross-modal data interpretation'], conclusion='Multimodal models offer versatile capabilities but present significant technical challenges.')]\n"
     ]
    }
   ],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Topic)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate a report on : {topic}\\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "response = chain.invoke({'topic':'LLMs'})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4182ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtopic(BaseModel):\n",
    "    name: str = Field(description=\"Name of the model type (e.g., Vision Models, Text Models)\") \n",
    "    advantages: List[str] = Field(description=\"List of advantages of this model type\") \n",
    "    disadvantages: List[str] = Field(description=\"List of disadvantages of this model type\") \n",
    "    conclusion: str = Field(description=\"Short conclusion about this model type\") \n",
    "    \n",
    "class Topic(BaseModel):\n",
    "    title: str = Field(description=\"Main topic name\") \n",
    "    intro: str = Field(description=\"Definition or introduction of the topic\")\n",
    "    categories: List[Subtopic] = Field(description=\"Different model types under this topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c92b3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: Large Language Models (LLMs)\n",
      "\n",
      "Introduction:\n",
      "Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to perform a wide range of tasks, including text generation, translation, summarization, and question-answering. They leverage deep learning techniques to understand and generate human-like text.\n",
      "\n",
      "Model Type: Text Models\n",
      "Advantages:\n",
      "  - Natural language understanding and generation\n",
      "  - Versatile for various text-based tasks\n",
      "  - High accuracy in language-related tasks\n",
      "Disadvantages:\n",
      "  - Limited to text processing, lacking visual/audio capabilities\n",
      "  - Potential for generating biased or inaccurate content\n",
      "  - Requires extensive training data for optimal performance\n",
      "Conclusion: Text models are essential for language-based applications but have limitations in handling non-text data.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model Type: Vision Models\n",
      "Advantages:\n",
      "  - Capable of processing and understanding visual data\n",
      "  - Effective in image recognition and description tasks\n",
      "  - Can integrate with text models for multimodal applications\n",
      "Disadvantages:\n",
      "  - Require large annotated image datasets for training\n",
      "  - Higher computational resources compared to text models\n",
      "  - Potential for biased or inaccurate visual interpretations\n",
      "Conclusion: Vision models expand the capabilities of LLMs in multimedia tasks but demand significant resources.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model Type: Code Models\n",
      "Advantages:\n",
      "  - Specialized in understanding and generating programming code\n",
      "  - Assist in debugging, code completion, and documentation\n",
      "  - Support multiple programming languages\n",
      "Disadvantages:\n",
      "  - May produce syntactically correct but semantically incorrect code\n",
      "  - Limited to code-related tasks, not general-purpose\n",
      "  - Requires domain-specific training data\n",
      "Conclusion: Code models are invaluable for developers but have constraints in broader applications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model Type: Multimodal Models\n",
      "Advantages:\n",
      "  - Integrate text, images, audio, and video processing\n",
      "  - Enable complex applications like virtual assistants and content creation\n",
      "  - Provide a more holistic understanding of data\n",
      "Disadvantages:\n",
      "  - High computational complexity and resource demands\n",
      "  - Challenging to train and fine-tune effectively\n",
      "  - Potential for errors in cross-modal data interpretation\n",
      "Conclusion: Multimodal models offer versatile capabilities but present significant technical challenges.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Topic: {response.title}\n",
    "\n",
    "Introduction:\n",
    "{response.intro}\n",
    "\"\"\")\n",
    "\n",
    "for category in response.categories:\n",
    "    print(f\"Model Type: {category.name}\")\n",
    "    \n",
    "    print(\"Advantages:\")\n",
    "    for adv in category.advantages:\n",
    "        print(f\"  - {adv}\")\n",
    "    \n",
    "    print(\"Disadvantages:\")\n",
    "    for disadv in category.disadvantages:\n",
    "        print(f\"  - {disadv}\")\n",
    "    \n",
    "    print(f\"Conclusion: {category.conclusion}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
